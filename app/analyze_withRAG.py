import pandas as pd
import ollama
import json
import time
from difflib import SequenceMatcher
from utils import load_data,product_list,select_file,question_list,contains_english,translate_to_chinese,df_to_documents,build_faiss_index,search_similar_documents 
from tkinter import filedialog, Tk
import os
from sentence_transformers import SentenceTransformer
from utils import load_faiss_index



# åˆå§‹åŒ– Ollama å®¢æˆ¶ç«¯
client = ollama.Client()

# å…¨åŸŸè®Šæ•¸å­˜æ”¾è³‡æ–™
json_file_path = None
df = None
summary = None
documents = None
# å»ºç«‹æ¨¡å‹
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

output_dir = "test_results"
def load_example_log():
    """è®€å–éå»ç¯„ä¾‹ç´€éŒ„ï¼Œè½‰ç‚º few-shot èªæ–™"""
    log_path = os.path.join("test_results", "test_results.csv")
    if not os.path.exists(log_path):
        return ""

    df_log = pd.read_csv(log_path)

    examples = df_log.head(10)  # å–å‰10ç­†ç¯„ä¾‹

    example_text = ""
    for i, row in examples.iterrows():
        example_text += f"""
        ã€ç¯„ä¾‹ {i+1}ã€‘
        å•é¡Œï¼š{row['Question']}
        ç­”æ¡ˆï¼š{row['Expected Answer']}
        éŒ¯èª¤å›ç­”ï¼š{row['Generated Answer']}

"""
    return example_text


def similarity(generated_answer, expected_answer):
    """
    æ ¹æ“šé æœŸç­”æ¡ˆä¸­æœ‰å¤šå°‘å­—è¢«å›æ‡‰æ­£ç¢ºå‘½ä¸­ï¼Œæ±ºå®šç›¸ä¼¼åº¦ã€‚
    - å®Œå…¨åŒ…å« âœ 1.0
    - å°åˆ°éƒ¨åˆ† âœ 0.3 ~ 0.9ï¼ˆä¾å­—å…ƒå‘½ä¸­æ¯”ä¾‹ï¼‰
    - å®Œå…¨æ²’å°åˆ° âœ 0.0
    """
    matched_chars = sum(1 for c in expected_answer if c in generated_answer)
    total_chars = len(expected_answer)
    if total_chars == 0:
        return 0.0  # é¿å…é™¤ä»¥é›¶çš„éŒ¯èª¤
    return matched_chars / total_chars


def load_json():
    global json_file_path, df, summary
    file_path = select_file()
    if file_path:
        df = load_data(file_path)
        if df is not None:
            global documents, index2, docs, embedding_model
            json_file_path = file_path
            summary = df.describe(include='all').to_string()
            print("âœ… è³‡æ–™æ‘˜è¦å·²ç”Ÿæˆï¼")
            documents = df_to_documents(df)
           
            #å¦‚æœç¬¬ä¸€æ¬¡åŸ·è¡Œ å‰‡æ³¨è§£é€™ä¸€è¡Œ
            index2, docs, embedding_model = load_faiss_index()

            # å¦‚æœç¬¬ä¸€æ¬¡åŸ·è¡Œ å‰‡å–æ¶ˆæ³¨è§£é€™ä¸€è¡Œ
            # index2, docs, embedding_model = build_faiss_index(documents)
            print("âœ… FAISS å‘é‡ç´¢å¼•å·²å»ºç«‹ï¼")


        else:
            print("âŒ è³‡æ–™åŠ è¼‰å¤±æ•—ï¼")
            exit()
    else:
        print("âš ï¸ æœªé¸æ“‡æª”æ¡ˆï¼Œç¨‹å¼çµæŸã€‚")
        exit()







def filter_df_by_question(df, question, product_list): #ç”¢å“é—œéµå­—æå–
    for name in product_list:
        if name in question:
            filtered = df[df["ç”¢å“åç¨±"].str.contains(name, na=False)]
            print(f"åµæ¸¬åˆ°ç”¢å“åç¨±ï¼š{name}ï¼Œå…± {len(filtered)} ç­†")
            print(filtered.head(0).to_string(index=False))  # åˆ—å°å‡ºç¯©é¸çµæœ
            return filtered, name

    return df.head(10000), None  

def extract_keyword(question): #å•é¡Œé—œéµå­—æå–
    for name in question_list:
        if name in question:
            print(f"åµæ¸¬åˆ°å•é¡Œé—œéµå­—ï¼š{name}")
            return name
    return None



def generate_prompts(question):
    filtered_df, matched_name = filter_df_by_question(df, question, product_list)
    table_text = filtered_df.to_markdown(index=False)
    matched_question = extract_keyword(question)
    
    #æŸ¥è©¢éå»ç¯„ä¾‹ç´€éŒ„
    example_text = load_example_log()

    # æŸ¥è©¢æœ€ç›¸é—œçš„æ–‡ä»¶æ®µè½
    top_k_docs = search_similar_documents(question, index2, docs, embedding_model, top_k=5)
    print(f"ğŸ”¢ æ“·å–æ®µè½æ•¸é‡ï¼š{len(top_k_docs)}")

    # å°‡çµæœåˆä½µæˆä¸€æ®µæ–‡å­—
    retrieved_context = "\n".join(top_k_docs)

    print("ğŸ” æ“·å–åˆ°çš„ç›¸é—œè³‡æ–™ç‰‡æ®µï¼š")
    print(retrieved_context)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½è³‡æ–™çµ±è¨ˆåŠ©ç†ï¼Œè«‹æ ¹æ“šè¡¨æ ¼ä¸­çš„è³‡æ–™å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š

    - Follow the dataï¼Œyou are not allowed to predict or guess.
    - Your answer must be in traditional Chinese.
    - Every row in the data represents a product.
    - If there aren't any relative information in the data,please answerï¼šã€ŒNo informationã€ã€‚
    - It is forbidden to give the answer which is not in the data, and not to repeat the question.
    - Here are some examples of the questions , correct answers and wrong answers,please avoid answering like the wrong answers:
{example_text}
"""

    user_prompt = f"""
        Please answer the question ï¼š
        questionï¼š{question}
        here is the data summaryï¼š
        {summary}
       
        here is the dataï¼š
        {retrieved_context}
"""

    return system_prompt, user_prompt


def generate_answer(system_prompt, user_prompt):
    try:
        response = client.chat(
            model="llama3",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=True
        )
        raw = ''.join([chunk["message"]["content"] for chunk in response])
        
        # å¦‚æœåŒ…å«è‹±æ–‡ï¼Œè‡ªå‹•ç¿»è­¯
        if contains_english(raw):
            print("ğŸŒ åµæ¸¬åˆ°è‹±æ–‡ï¼Œè‡ªå‹•ç¿»è­¯ä¸­...")
            translated = translate_to_chinese(raw)
            return translated
        return raw
    except Exception as e:
        return f"âŒ å›æ‡‰éŒ¯èª¤: {str(e)}"



def run_test_with_prompts(question, expected_answer):
    system_prompt, user_prompt = generate_prompts(question)
    start_time = time.time()
    response = generate_answer(system_prompt, user_prompt)
    end_time = time.time()

    acc = similarity(response, expected_answer)
    duration = end_time - start_time

    print(f"ğŸ” æ¸¬è©¦å•é¡Œï¼š{question}")
    print(f"ğŸ“ å›æ‡‰ï¼š{response}")
    print(f"ğŸ¯ ç›¸ä¼¼åº¦ï¼š{acc:.2f}")
    print(f"â³ è€—æ™‚ï¼š{duration:.2f} ç§’")
    print("=" * 80)

    return {
        "Question": question,
        "Expected Answer": expected_answer,
        "Generated Answer": response,
        "Accuracy": acc,
        "Duration": duration,
        "Pass": acc >= 0.8
    }

if __name__ == "__main__":
    load_json()

    test_cases = {
        "ç´…å¥¶æ²¹å…±æœ‰å¤šå°‘é¡†ï¼Ÿ": "3659é¡†",
        "ç´…ç«ç„°å…±æœ‰å¤šå°‘é¡†ï¼Ÿ": "5105é¡†",
        "ç´…ç«ç„°çš„åƒ¹æ ¼æ˜¯å¤šå°‘ï¼Ÿ": "å¾è³‡æ–™ä¸­ç„¡æ³•å¾—çŸ¥åƒ¹æ ¼ï¼Œå› çˆ²ç„¡æ­¤æ¬„ä½",
        "ç”¢å“ç·¨è™Ÿç‚º1101æ˜¯å“ªå€‹ç”¢å“ï¼Ÿ": "ç”¢å“ç·¨è™Ÿç‚º1101çš„ç”¢å“åŒ…å«ç´…ç«ç„°ï¼Œç´…ç‹ï¼Œå¥¶æ³¢ï¼Œç¶ æ²ç­‰ç”¢å“",
        "ç”¢å“ç·¨è™Ÿç‚º1102æ˜¯å“ªå€‹ç”¢å“ï¼Ÿ": "ç”¢å“ç·¨è™Ÿç‚º1102çš„ç”¢å“åŒ…å«ç´…ç«ç„°ï¼Œç´…ç‹ï¼Œå¥¶æ³¢ï¼Œç¶ æ²ç­‰ç”¢å“",
        "çµ±è¨ˆæœ€å¤šçš„ç”¢å“æ˜¯å“ªå…©ç¨®ï¼Ÿ": "ç´…ç«ç„°ï¼Œç¶ æ©¡",
        "çµ±è¨ˆæœ€å°‘çš„ç”¢å“æ˜¯å“ªå…©ç¨®ï¼Ÿ": "å¥¶æ²¹æ³¢å£«é “ï¼Œç´…è”“å¿ƒ",
        "å¤§éƒ¨åˆ†çš„ç”¢å“ç‹€æ…‹æ˜¯ä»€éº½":"å¤§éƒ¨åˆ†çš„ç”¢å“ç‹€æ…‹æ˜¯ç¨®æ¤ä¸­",
        "ç”¢å“çš„ç¨®æ¤æ™‚é–“åˆ†ä½ˆï¼Ÿ" :"ç”¢å“çš„ç¨®æ¤æ™‚é–“åˆ†ä½ˆåœ¨ 40 åˆ° 70å¤©ä¹‹é–“ï¼Œå¤§éƒ¨åˆ†åœ¨42å¤©å·¦å³",
        "æœ€æ—©çµ±è¨ˆçš„è³‡æ–™çš„æ˜¯å“ªä¸€ç­†" :"æœ€æ—©çµ±è¨ˆçš„è³‡æ–™æ˜¯ 2022-03-03",
        "æœ€æ™šçµ±è¨ˆçš„è³‡æ–™çš„æ˜¯å“ªä¸€ç­†" :"æœ€æ™šçµ±è¨ˆçš„è³‡æ–™æ˜¯ 2025-03-24",
        "ç”¢å“ç·¨è™Ÿç‚º1101çš„ç”¢å“ç‹€æ…‹æ˜¯ä»€éº½ï¼Ÿ":"ç”¢å“ç·¨è™Ÿç‚º1101çš„ç”¢å“ç‹€æ…‹æ˜¯ç¨®æ¤ä¸­",
        "ç¶ æ©¡çš„ç”¢å“ç·¨è™Ÿæ˜¯å¤šå°‘ï¼Ÿ":"ç¶ æ©¡çš„ç”¢å“ç·¨è™Ÿå¾5160åˆ°8126éƒ½æœ‰åˆ†ä½ˆ"


    }

    results = []
    for question, expected in test_cases.items():
        results.append(run_test_with_prompts(question, expected))

    
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "test_results.csv")
    pd.DataFrame(results).to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False, encoding="utf-8-sig")
    print(f"âœ… æ¸¬è©¦å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ï¼š{output_path}")
