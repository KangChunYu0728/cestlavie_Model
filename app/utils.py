import pandas as pd
import json
import numpy as np  # éœ€è¦é€™å€‹ä¾†åˆ¤æ–· NaN
from tkinter import filedialog, Tk
import os
from deep_translator import GoogleTranslator  # pip install deep-translator
import re
from sentence_transformers import SentenceTransformer
import faiss
import pickle

# æ‰€æœ‰ç”¢å“åç¨±ï¼ˆä¾†è‡ªä½ åœ–ç‰‡ï¼‰
product_list = [
    "å¥¶æ²¹æ³¢å£«é “", "å¥¶æ³¢", "ç‰èŠ™è“‰", "å†°èŠ±", "è²æ¯”èµ", "ç´…ç«ç„°", "ç´…å¥¶æ²¹",
    "ç´…ç‹", "ç´…å½¤", "ç´…æ©™", "ç´…ç”œè„†", "ç´…èŠ½å¿ƒ", "ç´…ç¶ ",
    "è‹±è²æ¯”èµ", "è‹±è²æ¯”èµ(ä¸åˆ†å“ç¨®)", "æé¾ç”˜è—", "æé¾ç¾½è¡£ç”˜è—", "ç²‰å«©å¤©ä½¿",
    "æ²è‘‰ç”˜è—", "æ²è‘‰ç¾½è¡£ç”˜è—", "èŠè‹£", "è©¦ç¨®", "ç¶ æ°´æ™¶", "ç¶ ç«ç„°",
    "ç¶ ç‹", "ç¶ ç”œè„†", "ç¶ æ©¡","ç¶ è˜¿è”“","ç¶ å¯¶çŸ³"
]

question_list = [ "ç”¢å“åç¨±","ç”¢å“ç·¨è™Ÿ", "ç¨®æ¤æ—¥æœŸ", "æ¡æ”¶æ—¥æœŸ", "ç‹€æ…‹","å¤šå°‘é¡†","å¤šå°‘æ–¤","å¤šå°‘å…¬æ–¤",
                 "å¤šå°‘å…¬å…‹","å¤šå°‘ç‰‡","å¤šå°‘æ ª","å¤šå°‘æ£µ","å¤šå°‘æ ªæ•¸","å¤šå°‘ç‰‡æ•¸","å¤šå°‘æ–¤æ•¸","å¤šå°‘å…¬æ–¤æ•¸","å¤šå°‘å…¬å…‹æ•¸",
                 "æœ€å¤š","æœ€å°‘","å¹³å‡","ä¸­ä½æ•¸","æ¨™æº–å·®","è®Šç•°æ•¸","æœ€å¤§å€¼","æœ€å°å€¼",
                 "ç¸½å’Œ","ç¸½è¨ˆ","ç¸½æ•¸","ç¸½é‡","ç¸½é‡é‡","ç¸½æ–¤æ•¸","ç¸½å…¬æ–¤æ•¸","ç¸½å…¬å…‹æ•¸",
                 "åƒ¹æ ¼","å–®åƒ¹","å”®åƒ¹"
]


# è®€å– JSON æª”æ¡ˆï¼ˆæ‰‹å‹•é¸æ“‡ï¼‰
def select_file():
    root = Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="é¸æ“‡ JSON æª”æ¡ˆ",
        filetypes=[("JSON files", "*.json")]
    )
    return file_path




#è®€å– JSON ä¸¦éæ¿¾æ¬„ä½é½Šå…¨ã€ç„¡ç©ºå€¼çš„è³‡æ–™
def load_data(file_path):
    """è®€å– JSON ä¸¦éæ¿¾æ¬„ä½é½Šå…¨ã€ç„¡ç©ºå€¼çš„è³‡æ–™"""
    try:
        print(f"ğŸ“‚ æ­£åœ¨è®€å–æª”æ¡ˆ: {file_path}")

        with open(file_path, "r", encoding="utf-8") as file:
            json_data = json.load(file)

        required_columns = ["ç”¢å“ç·¨è™Ÿ", "ç”¢å“åç¨±", "ç¨®æ¤æ—¥æœŸ", "æ¡æ”¶æ—¥æœŸ", "ç‹€æ…‹"]
        if "Sheet1" not in json_data:
            raise ValueError("âŒ JSON æ ¼å¼éŒ¯èª¤: æ‰¾ä¸åˆ° 'Sheet1'")

        valid_records = []
        invalid_count = 0

        for record in json_data["Sheet1"]:
            # ç¢ºä¿æ¬„ä½éƒ½å­˜åœ¨
            if all(col in record for col in required_columns):
                # ç¢ºä¿æ¬„ä½å…§å®¹ä¸æ˜¯ Noneã€ç©ºå­—ä¸²æˆ– NaN
                if all(
                    record[col] not in [None, ""] and not pd.isna(record[col])
                    for col in required_columns
                ):
                    valid_records.append(record)
                else:
                    invalid_count += 1
            else:
                invalid_count += 1

        if not valid_records:
            raise ValueError("âŒ æ²’æœ‰ç¬¦åˆè¦æ±‚çš„è³‡æ–™è¨˜éŒ„")

        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(valid_records)} ç­†æœ‰æ•ˆè³‡æ–™")
        if invalid_count > 0:
            print(f"âš ï¸ æ’é™¤ {invalid_count} ç­†å«ç©ºå€¼æˆ–ä¸å®Œæ•´çš„è³‡æ–™")

        # è½‰æˆ DataFrame
        df = pd.DataFrame(valid_records)
        df["ç¨®æ¤æ—¥æœŸ"] = pd.to_datetime(df["ç¨®æ¤æ—¥æœŸ"])
        df["æ¡æ”¶æ—¥æœŸ"] = pd.to_datetime(df["æ¡æ”¶æ—¥æœŸ"])
        df["ç¨®æ¤æ™‚é–“ï¼ˆæ—¥ï¼‰"] = (df["æ¡æ”¶æ—¥æœŸ"] - df["ç¨®æ¤æ—¥æœŸ"]).dt.days

        return df

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    

#ç¿»è­¯
def contains_english(text):
    """æª¢æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡"""
    return re.search(r'[a-zA-Z]', text) is not None

def translate_to_chinese(text):
    """ç¿»è­¯è‹±æ–‡ â†’ ä¸­æ–‡"""
    try:
        return GoogleTranslator(source='auto', target='zh-TW').translate(text)
    except Exception as e:
        print(f"âš ï¸ ç¿»è­¯å¤±æ•—ï¼š{str(e)}")
        return text  # å›å‚³åŸæ–‡


def df_to_documents(df):
    documents = []
    for _, row in df.iterrows():
        doc = f"ç”¢å“ç·¨è™Ÿç‚º {row['ç”¢å“ç·¨è™Ÿ']}ï¼Œåç¨±æ˜¯ {row['ç”¢å“åç¨±']}ï¼Œç¨®æ¤æ—¥æœŸæ˜¯ {row['ç¨®æ¤æ—¥æœŸ']}ï¼Œæ¡æ”¶æ—¥æœŸæ˜¯ {row['æ¡æ”¶æ—¥æœŸ']}ï¼Œç‹€æ…‹ç‚º {row['ç‹€æ…‹']}ï¼Œç¨®æ¤æ™‚é–“æ˜¯ {row['ç¨®æ¤æ™‚é–“']}ã€‚"
        documents.append(doc)
    return documents

import pickle

def build_faiss_index(documents, index_path="faiss_index.index", doc_path="documents.pkl"):
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    embeddings = model.encode(documents)
    faiss.normalize_L2(embeddings)

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    index.add(embeddings)

    # å„²å­˜ index åˆ°æª”æ¡ˆ
    faiss.write_index(index, index_path)

    # ä¿®æ­£é€™è£¡ â†“â†“â†“â†“â†“
    with open(doc_path, "wb") as f:
        pickle.dump({
            "documents": documents,
            "model_name": model._first_module().auto_model.config._name_or_path  # â† ä¿®æ­£é»
        }, f)

    print(f"âœ… FAISS Index èˆ‡æ–‡ä»¶å„²å­˜å®Œæˆï¼Œå…± {index.ntotal} ç­†ã€‚")
    return index, documents, model



def search_similar_documents(question, index, documents, model, top_k=5):
    """æŸ¥è©¢èˆ‡å•é¡Œæœ€ç›¸é—œçš„æ–‡ä»¶æ®µè½"""
    query_embedding = model.encode([question])
    faiss.normalize_L2(query_embedding)

    scores, indices = index.search(query_embedding, top_k)

    results = []
    for idx in indices[0]:
        if 0 <= idx < len(documents):
            results.append(documents[idx])
    return results


def load_faiss_index(index_path="faiss_index.index", doc_path="documents.pkl"):
    if not os.path.exists(index_path) or not os.path.exists(doc_path):
        return None, None, None

    # è¼‰å…¥ index
    index = faiss.read_index(index_path)

    # è¼‰å…¥ documents å’Œæ¨¡å‹
    with open(doc_path, "rb") as f:
        data = pickle.load(f)
        documents = data["documents"]
        model = SentenceTransformer(data["model_name"])

    print(f"ğŸ“¦ å·²å¾æª”æ¡ˆè¼‰å…¥ FAISS Indexï¼Œå…± {index.ntotal} ç­†ã€‚")
    return index, documents, model

