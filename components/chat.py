# components/chat.py
import os
import streamlit as st
from openai import OpenAI
import pandas as pd

# ---------- API Key è®€å–ï¼ˆå„ªå…ˆç’°å¢ƒè®Šæ•¸ï¼Œå…¶æ¬¡ st.secretsï¼›é¿å… secrets æª”ç¼ºå¤±æ™‚å ±éŒ¯ï¼‰ ----------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    try:
        api_key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        api_key = None

if not api_key:
    st.error(
        "âŒ æ‰¾ä¸åˆ° OpenAI API Keyã€‚\n"
        "è«‹åœ¨æœ¬æ©Ÿå»ºç«‹ `.streamlit/secrets.toml` æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸ `OPENAI_API_KEY`ï¼›\n"
        "éƒ¨ç½²æ–¼ Streamlit Cloud æ™‚ï¼Œè«‹åœ¨ App â†’ Settings â†’ Secrets è¨­å®šã€‚"
    )
    st.stop()

client = OpenAI(api_key=api_key)


# ---------- ç¯„ä¾‹å•é¡ŒæŒ‰éˆ• ----------
def render_example_questions():
    st.caption("ğŸ’¡ ç¯„ä¾‹å•é¡Œï¼ˆé»ä¸€ä¸‹å³å¯æå•ï¼‰")

    basic = [
        "å“ªå€‹ç”¢å“çš„å¹³å‡ç¨®æ¤æ™‚é–“æœ€é•·ï¼Ÿç‚ºä»€éº¼ï¼Ÿ",
        "è«‹ç”¨ 3 é»ç¸½çµé€™ä»½è³‡æ–™çš„é‡é»è¶¨å‹¢ã€‚",
        "åˆ—å‡ºå‰ 5 åå¹³å‡ç¨®æ¤æ™‚é–“æœ€çŸ­çš„ç”¢å“èˆ‡å…¶æ•¸å€¼ã€‚",
        "å“ªäº›ç”¢å“åœ¨æœ€è¿‘ä¸€é€±æ•¸é‡è®Šå‹•æœ€å¤§ï¼Ÿ",
    ]
    charts = [
        "è«‹ç”¨è¡¨æ ¼å½™æ•´ï¼šæ¯å€‹ç”¢å“çš„å¹³å‡ç¨®æ¤æ™‚é–“èˆ‡æ•¸é‡çµ±è¨ˆã€‚",
        "å¯«ä¸€æ®µ 100 å­—å…§çš„åœ–è¡¨è§£è®€ï¼šç‹€æ…‹æ¯”ä¾‹åœ–å‘Šè¨´æˆ‘å€‘ä»€éº¼ï¼Ÿ",
        "å¹«æˆ‘æ‰¾å‡ºå¯èƒ½çš„ç•°å¸¸å€¼ï¼Œä¸¦è§£é‡‹ç†ç”±ã€‚",
        "æ¯”è¼ƒ A èˆ‡ B å“é …åœ¨è¿‘æœŸçš„å¹³å‡ã€æ¥µå€¼èˆ‡è¶¨å‹¢ã€‚",
    ]
    advanced = [
        "æå‡º 3 å€‹å¯è¡Œçš„ç‡Ÿé‹å»ºè­°ï¼Œä¸¦å¼•ç”¨è³‡æ–™ä¸­çš„æ•¸å­—ä½è­‰ã€‚",
        "å¦‚æœåªæœ‰ 1 å¼µæŠ•å½±ç‰‡ï¼Œè¦å¦‚ä½•è¬›æ¸…æ¥šé€™ä»½è³‡æ–™ï¼Ÿ",
        "åˆ—å‡ºä¸‹ä¸€æ­¥æ‡‰è©²è¿½è¹¤çš„ 3 å€‹æŒ‡æ¨™ä¸¦èªªæ˜åŸå› ã€‚",
        "å¾æ¡è³¼è§’åº¦çœ‹ï¼Œé€™ä»½è³‡æ–™æœ‰å“ªäº›é¢¨éšªè¨Šè™Ÿï¼Ÿ",
    ]

    tabs = st.tabs(["åŸºç¤", "åœ–è¡¨/åˆ†æ", "é€²éš/æ±ºç­–"])
    for tab, questions in zip(tabs, [basic, charts, advanced]):
        with tab:
            cols = st.columns(2)
            for i, q in enumerate(questions):
                if cols[i % 2].button(q, key=f"sug_{tab._active_index}_{i}"):
                    # å°‡é»æ“Šçš„é¡Œç›®è¨˜åˆ° session_stateï¼Œç¨å¾Œç›´æ¥ç•¶ä½œæœ¬å›åˆè¼¸å…¥
                    st.session_state["prefill_question"] = q
                    st.rerun()


# ---------- DF æ‘˜è¦ï¼ˆæ§åˆ¶ Token ç”¨é‡ï¼‰ ----------
def _summarize_df(df: pd.DataFrame, max_lines: int = 40) -> str:
    if df is None or df.empty:
        return ""
    desc = df.describe(include="all").astype(str).to_string()
    lines = desc.splitlines()
    return "\n".join(lines[:max_lines])


# ---------- ä¸»èŠå¤©ä»‹é¢ ----------
def chat_interface(df: pd.DataFrame):
    if df is None or df.empty:
        st.warning("âš ï¸ ç„¡æ³•é¡¯ç¤ºèŠå¤©ï¼Œå› ç‚ºè³‡æ–™å°šæœªè¼‰å…¥ã€‚")
        return

    # ä¸€æ¬¡æ€§è¨ˆç®—ä¸¦å¿«å– DF æ‘˜è¦
    if "df_summary" not in st.session_state:
        st.session_state.df_summary = _summarize_df(df, max_lines=40)

    # åˆå§‹åŒ–å°è©±æ­·å²
    if "messages" not in st.session_state:
        st.session_state.messages = []  # list[{"role": "user"|"assistant", "content": str}]

    st.markdown("è«‹è¼¸å…¥æ‚¨çš„åˆ†æå•é¡Œï¼ŒAI å°‡æ ¹æ“šè³‡æ–™æ‘˜è¦èˆ‡å°è©±è„ˆçµ¡å›ç­”ï¼š")

    # æ¸²æŸ“æ­·å²
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # æ“ä½œåˆ—
    cols = st.columns(2)
    with cols[0]:
        if st.button("ğŸ§¹ æ¸…é™¤å°è©±", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with cols[1]:
        add_df_ctx = st.toggle("æ¯å›åˆåŠ å…¥è³‡æ–™æ‘˜è¦ï¼ˆRAGï¼‰", value=True, help="é—œé–‰å¯æ¸›å°‘ Token ç”¨é‡ã€‚")

    # ç¯„ä¾‹å•é¡Œï¼ˆé»æ“Šå¾Œæœƒé€é session_state é€å‡ºï¼‰
    render_example_questions()

    # ä½¿ç”¨è€…è¼¸å…¥
    user_text = st.chat_input("ğŸ” ç›´æ¥ç™¼å•ï¼ˆè‡ªç”±è¼¸å…¥ï¼‰")

    # è‹¥å‰›æŒ‰äº†ç¯„ä¾‹å•é¡ŒæŒ‰éˆ•ï¼Œå„ªå…ˆä½¿ç”¨è©²é¡Œç›®ç•¶ä½œæœ¬å›åˆè¼¸å…¥
    if st.session_state.get("prefill_question"):
        user_text = st.session_state.pop("prefill_question")

    if not user_text:
        return

    # åŠ å…¥æ­·å²ä¸¦é¡¯ç¤º
    st.session_state.messages.append({"role": "user", "content": user_text})
    with st.chat_message("user"):
        st.markdown(user_text)

    # æº–å‚™é€å…¥æ¨¡å‹çš„å…§å®¹ï¼ˆè‡ªç”±å¼ï¼›ä¸åŠ  system promptï¼‰
    if add_df_ctx and st.session_state.df_summary:
        model_input = f"{user_text}\n\n[è³‡æ–™æ‘˜è¦]\n{st.session_state.df_summary}"
    else:
        model_input = user_text

    # ä¸²æµå›è¦†
    with st.chat_message("assistant"):
        placeholder = st.empty()
        collected = []
        try:
            with client.responses.stream(
                model="gpt-4o-mini",
                input=model_input,
                temperature=0.2,
            ) as stream:
                for event in stream:
                    if event.type == "response.output_text.delta":
                        collected.append(event.delta)
                        placeholder.markdown("".join(collected))
                assistant_text = stream.get_final_response().output_text
        except Exception as e:
            assistant_text = f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

        placeholder.markdown(assistant_text)

    # ä¿å­˜åŠ©ç†å›è¦†
    st.session_state.messages.append({"role": "assistant", "content": assistant_text})

    # è£å‰ªæ­·å²é•·åº¦ï¼ˆé¿å… context éå¤§ï¼‰
    max_msgs = 12
    if len(st.session_state.messages) > max_msgs:
        st.session_state.messages = st.session_state.messages[-max_msgs:]
