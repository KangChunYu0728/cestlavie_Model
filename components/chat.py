# component/chat.py
import streamlit as st
from openai import OpenAI
import pandas as pd

# Use secrets for key (works locally + Streamlit Cloud)
api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ æ‰¾ä¸åˆ° OpenAI API Keyã€‚è«‹åœ¨ .streamlit/secrets.toml æˆ–ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®š `OPENAI_API_KEY`ã€‚")
    st.stop()
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

def _summarize_df(df: pd.DataFrame, max_lines: int = 40) -> str:
    """Compact DF summary to control token usage."""
    if df is None or df.empty:
        return ""
    desc = df.describe(include="all").astype(str).to_string()
    lines = desc.splitlines()
    return "\n".join(lines[:max_lines])

def chat_interface(df: pd.DataFrame):
    if df is None or df.empty:
        st.warning("âš ï¸ ç„¡æ³•é¡¯ç¤ºèŠå¤©ï¼Œå› ç‚ºè³‡æ–™å°šæœªè¼‰å…¥ã€‚")
        return

    # One-time compute & cache a lightweight DF summary
    if "df_summary" not in st.session_state:
        st.session_state.df_summary = _summarize_df(df, max_lines=40)

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []  # list[{"role": "user"|"assistant", "content": str}]

    st.markdown("æ­¡è¿å•ä»»ä½•è·Ÿç¨®æ¤æœ‰é—œå•é¡Œï¼ŒAI å°‡æ ¹æ“šè³‡æ–™æ‘˜è¦èˆ‡å°è©±è„ˆçµ¡å›ç­”ï¼š")

    # Render history
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Controls
    cols = st.columns(2)
    with cols[0]:
        if st.button("ğŸ§¹ æ¸…é™¤å°è©±", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with cols[1]:
        add_df_ctx = st.toggle("æ¯å›åˆåŠ å…¥è³‡æ–™æ‘˜è¦ï¼ˆRAGï¼‰", value=True, help="é—œé–‰å¯æ¸›å°‘ Token ç”¨é‡ã€‚")

    # User input
    user_text = st.chat_input("ğŸ” ç›´æ¥ç™¼å•ï¼ˆè‡ªç”±è¼¸å…¥ï¼‰")
    if not user_text:
        return

    # Append user message to history
    st.session_state.messages.append({"role": "user", "content": user_text})
    with st.chat_message("user"):
        st.markdown(user_text)

    # Prepare model input (free-form; no system msg)
    if add_df_ctx and st.session_state.df_summary:
        model_input = f"{user_text}\n\n[è³‡æ–™æ‘˜è¦]\n{st.session_state.df_summary}"
    else:
        model_input = user_text

    # Stream the assistant reply
    with st.chat_message("assistant"):
        placeholder = st.empty()
        collected = []

        try:
            with client.responses.stream(
                model="gpt-4o-mini",
                input=model_input,
                temperature=0.2,
            ) as stream:
                for event in stream:
                    if event.type == "response.output_text.delta":
                        collected.append(event.delta)
                        # Efficient incremental render
                        placeholder.markdown("".join(collected))
                # Final text
                assistant_text = stream.get_final_response().output_text
        except Exception as e:
            assistant_text = f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

        placeholder.markdown(assistant_text)

    # Save assistant reply
    st.session_state.messages.append({"role": "assistant", "content": assistant_text})

    # Optional: trim history to avoid huge context (keeps last 12 messages = 6 turns)
    max_msgs = 12
    if len(st.session_state.messages) > max_msgs:
        st.session_state.messages = st.session_state.messages[-max_msgs:]
